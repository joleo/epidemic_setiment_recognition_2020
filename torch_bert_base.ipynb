{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('data/nCoV_100k_train.labled.csv',header=0)\n",
    "test = pd.read_csv('data/nCov_10k_test.csv',header=0)\n",
    "train = train[(train['情感倾向'].isin(['-1','0','1']))]\n",
    "train = train[~train.index.isin(['35202','41624','33936'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ['微博id','微博中文内容','微博发布时间','情感倾向']\n",
    "train['情感倾向'] = train['情感倾向'].astype('int')+1\n",
    "test['情感倾向'] = -1\n",
    "train = train[col]\n",
    "test = test[col]\n",
    "train.columns = ['id','title','content','label']\n",
    "test.columns = ['id','title','content','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['title'] = train['title'].astype('str')\n",
    "train['content'] = train['content'].astype('str')\n",
    "test['title'] = test['title'].astype('str')\n",
    "test['content'] = test['content'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.576689\n",
       "2    0.254149\n",
       "0    0.169162\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "train_df = train.copy()\n",
    "test_df = test.copy()\n",
    "train_df['title'] = train_df['title'].astype('str')\n",
    "train_df['content'] = train_df['content'].astype('str')\n",
    "X = np.array(train_df.index)\n",
    "y = train_df.loc[:,'label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 TRAIN: [    0     1     2 ... 99906 99907 99908] TEST: [    3     5     6 ... 99893 99902 99909]\n",
      "(79926, 4) (19984, 4)\n",
      "1 TRAIN: [    0     1     2 ... 99907 99908 99909] TEST: [    4     7     8 ... 99880 99887 99900]\n",
      "(79927, 4) (19983, 4)\n",
      "2 TRAIN: [    0     2     3 ... 99906 99908 99909] TEST: [    1     9    11 ... 99890 99895 99907]\n",
      "(79929, 4) (19981, 4)\n",
      "3 TRAIN: [    0     1     3 ... 99907 99908 99909] TEST: [    2    16    18 ... 99897 99904 99905]\n",
      "(79929, 4) (19981, 4)\n",
      "4 TRAIN: [    1     2     3 ... 99905 99907 99909] TEST: [    0    10    12 ... 99903 99906 99908]\n",
      "(79929, 4) (19981, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def generate_data(random_state = 24, is_pse_label=True):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    i = 0\n",
    "    for train_index, dev_index in skf.split(X, y):\n",
    "        print(i, \"TRAIN:\", train_index, \"TEST:\", dev_index)\n",
    "        DATA_DIR = \"./data/data_StratifiedKFold_{}/data_origin_{}/\".format(random_state,i)\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            os.makedirs(DATA_DIR)\n",
    "        tmp_train_df = train_df.iloc[train_index]\n",
    "        \n",
    "        tmp_dev_df = train_df.iloc[dev_index]\n",
    "        \n",
    "        test_df.to_csv(DATA_DIR+\"test.csv\")\n",
    "        if is_pse_label:\n",
    "            pse_dir = \"data_pse_{}/\".format(i)\n",
    "            pse_df = pd.read_csv(pse_dir+'train.csv')\n",
    "\n",
    "            tmp_train_df = pd.concat([tmp_train_df, pse_df],ignore_index=True,sort=False)\n",
    "            \n",
    "        tmp_train_df.to_csv(DATA_DIR + \"train.csv\")\n",
    "        tmp_dev_df.to_csv(DATA_DIR+\"dev.csv\")\n",
    "        print(tmp_train_df.shape, tmp_dev_df.shape)\n",
    "        i+=1\n",
    "\n",
    "generate_data(random_state=48, is_pse_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chinese_roberta_wwm_ext_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06/2020 10:29:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   Model name '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   loading file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "04/06/2020 10:29:10 - INFO - pytorch_transformers.modeling_utils -   loading weights file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "04/06/2020 10:29:18 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_muldropout not initialized from pretrained model: ['weights', 'fc.weight', 'fc.bias']\n",
      "04/06/2020 10:29:18 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_muldropout: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   content: ['01', '月', '01', '日', '23', ':', '50']\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   *** Example ***\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   idx: 0\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   guid: 4456072029125500\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   tokens: [CLS] 写 在 年 末 冬 初 孩 子 流 感 的 第 五 天 ， 我 们 仍 然 没 有 忘 记 热 情 拥 抱 这 2020 年 的 第 一 天 。 带 着 一 丝 迷 信 ， 早 晨 给 孩 子 穿 上 红 色 的 羽 绒 服 羽 绒 裤 ， 祈 祷 新 的 一 年 ， 孩 子 们 身 体 康 健 。 仍 然 会 有 一 丝 焦 虑 ， 焦 虑 我 的 孩 子 为 什 么 会 过 早 的 懂 事 ， 从 两 岁 多 开 始 关 注 我 的 情 绪 ， 会 深 沉 地 说 ： 妈 妈 ， 你 终 于 笑 了 ！ 这 句 话 像 刀 子 一 样 扎 入 我 ? 展 开 全 文 c [SEP] 01 月 01 日 23 : [SEP]\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   input_ids: 101 1091 1762 2399 3314 1100 1159 2111 2094 3837 2697 4638 5018 758 1921 8024 2769 812 793 4197 3766 3300 2563 6381 4178 2658 2881 2849 6821 8439 2399 4638 5018 671 1921 511 2372 4708 671 692 6837 928 8024 3193 3247 5314 2111 2094 4959 677 5273 5682 4638 5417 5309 3302 5417 5309 6175 8024 4857 4876 3173 4638 671 2399 8024 2111 2094 812 6716 860 2434 978 511 793 4197 833 3300 671 692 4193 5991 8024 4193 5991 2769 4638 2111 2094 711 784 720 833 6814 3193 4638 2743 752 8024 794 697 2259 1914 2458 1993 1068 3800 2769 4638 2658 5328 8024 833 3918 3756 1765 6432 8038 1968 1968 8024 872 5303 754 5010 749 8013 6821 1368 6413 1008 1143 2094 671 3416 2799 1057 2769 136 2245 2458 1059 3152 145 102 8146 3299 8146 3189 8133 131 102 0 0\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      "04/06/2020 10:29:25 - INFO - __main__ -   label: 1\n",
      "04/06/2020 10:32:24 - INFO - __main__ -   ***** Running training *****\n",
      "04/06/2020 10:32:24 - INFO - __main__ -     Num examples = 79926\n",
      "04/06/2020 10:32:24 - INFO - __main__ -     Batch size = 32\n",
      "04/06/2020 10:32:24 - INFO - __main__ -     Num steps = 3000\n",
      "  0%|                                                  | 0/3000 [00:00<?, ?it/s]04/06/2020 10:33:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/06/2020 10:33:10 - INFO - __main__ -     Num examples = 19984\n",
      "04/06/2020 10:33:10 - INFO - __main__ -     Batch size = 32\n",
      "Traceback (most recent call last):\n",
      "  File \"./bert_att__muldropout.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./bert_att__muldropout.py\", line 544, in main\n",
      "    labels=label_ids)\n",
      "  File \"/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/data/data01/liyang099/com/multi_setiment_reg/pytorch_transformers/modeling_bert.py\", line 1235, in forward\n",
      "    h = self.fc(dropout(feature))\n",
      "  File \"/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [32 x 768], m2: [2304 x 3] at /opt/conda/conda-bld/pytorch_1573049301898/work/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
      "  0%|                                                  | 0/3000 [00:47<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ./bert_att__muldropout.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_48/data_origin_0 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_48_gru1_0 \\\n",
    "--max_seq_length 155 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 32 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 3000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 13:21:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   Model name '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   loading file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "04/02/2020 13:21:53 - INFO - pytorch_transformers.modeling_utils -   loading weights file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "04/02/2020 13:22:01 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2020 13:22:01 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   content: ['01', '月', '01', '日', '23', ':', '50']\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   *** Example ***\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   idx: 0\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   guid: 4456072029125500\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   tokens: [CLS] 写 在 年 末 冬 初 孩 子 流 感 的 第 五 天 ， 我 们 仍 然 没 有 忘 记 热 情 拥 抱 这 2020 年 的 第 一 天 。 带 着 一 丝 迷 信 ， 早 晨 给 孩 子 穿 上 红 色 的 羽 绒 服 羽 绒 裤 ， 祈 祷 新 的 一 年 ， 孩 子 们 身 体 康 健 。 仍 然 会 有 一 丝 焦 虑 ， 焦 虑 我 的 孩 子 为 什 么 会 过 早 的 懂 事 ， 从 两 岁 多 开 始 关 注 我 的 情 绪 ， 会 深 沉 地 说 ： 妈 妈 ， 你 终 于 笑 了 ！ 这 句 话 像 刀 子 一 样 扎 入 我 ? 展 开 全 文 c [SEP] 01 月 01 日 23 : [SEP]\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   input_ids: 101 1091 1762 2399 3314 1100 1159 2111 2094 3837 2697 4638 5018 758 1921 8024 2769 812 793 4197 3766 3300 2563 6381 4178 2658 2881 2849 6821 8439 2399 4638 5018 671 1921 511 2372 4708 671 692 6837 928 8024 3193 3247 5314 2111 2094 4959 677 5273 5682 4638 5417 5309 3302 5417 5309 6175 8024 4857 4876 3173 4638 671 2399 8024 2111 2094 812 6716 860 2434 978 511 793 4197 833 3300 671 692 4193 5991 8024 4193 5991 2769 4638 2111 2094 711 784 720 833 6814 3193 4638 2743 752 8024 794 697 2259 1914 2458 1993 1068 3800 2769 4638 2658 5328 8024 833 3918 3756 1765 6432 8038 1968 1968 8024 872 5303 754 5010 749 8013 6821 1368 6413 1008 1143 2094 671 3416 2799 1057 2769 136 2245 2458 1059 3152 145 102 8146 3299 8146 3189 8133 131 102 0 0\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 13:22:07 - INFO - __main__ -   label: 1\n",
      "04/02/2020 13:25:12 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2020 13:25:12 - INFO - __main__ -     Num examples = 79927\n",
      "04/02/2020 13:25:12 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:25:12 - INFO - __main__ -     Num steps = 2500\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]04/02/2020 13:25:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:25:59 - INFO - __main__ -     Num examples = 19983\n",
      "04/02/2020 13:25:59 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:27:01 - INFO - __main__ -     eval_F1 = 0.2902363871186503\n",
      "04/02/2020 13:27:01 - INFO - __main__ -     eval_loss = 1.0510614093500203\n",
      "04/02/2020 13:27:01 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.2902363871186503\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.638:  32%|████████▉                   | 799/2500 [04:13<05:15,  5.39it/s]04/02/2020 13:29:26 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:29:26 - INFO - __main__ -     global_step = 200\n",
      "04/02/2020 13:29:26 - INFO - __main__ -     train loss = 0.638\n",
      "loss 0.5843:  64%|████████████████▋         | 1599/2500 [06:37<02:47,  5.39it/s]04/02/2020 13:31:50 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:31:50 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 13:31:50 - INFO - __main__ -     train loss = 0.5843\n",
      "04/02/2020 13:32:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:32:36 - INFO - __main__ -     Num examples = 19983\n",
      "04/02/2020 13:32:36 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:33:38 - INFO - __main__ -     eval_F1 = 0.7134874468105497\n",
      "04/02/2020 13:33:38 - INFO - __main__ -     eval_loss = 0.5608715224570741\n",
      "04/02/2020 13:33:38 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 13:33:38 - INFO - __main__ -     loss = 0.5843\n",
      "================================================================================\n",
      "Best F1 0.7134874468105497\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5674:  96%|████████████████████████▉ | 2399/2500 [10:51<00:18,  5.39it/s]04/02/2020 13:36:04 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:36:04 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 13:36:04 - INFO - __main__ -     train loss = 0.5674\n",
      "04/02/2020 13:36:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:36:50 - INFO - __main__ -     Num examples = 19983\n",
      "04/02/2020 13:36:50 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 13:37:52 - INFO - __main__ -     eval_F1 = 0.7238139960742004\n",
      "04/02/2020 13:37:52 - INFO - __main__ -     eval_loss = 0.5539993637095625\n",
      "04/02/2020 13:37:52 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 13:37:52 - INFO - __main__ -     loss = 0.5674\n",
      "================================================================================\n",
      "Best F1 0.7238139960742004\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5501: 100%|██████████████████████████| 2500/2500 [13:00<00:00,  3.20it/s]\n",
      "04/02/2020 13:38:14 - INFO - pytorch_transformers.modeling_utils -   loading weights file data/model_save/roberta_wwm_large_48_gru1_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7238139960742004\n",
      "/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ./bert_last3embedding_cls.py \\\n",
    "--model_type bert \\BertForSequenceClassification_muldropout\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_48/data_origin_1 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_48_gru1_1 \\\n",
    "--max_seq_length 155 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 64 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 2500 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 14:35:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   Model name '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   loading file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "04/02/2020 14:35:58 - INFO - pytorch_transformers.modeling_utils -   loading weights file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "04/02/2020 14:36:05 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2020 14:36:05 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   content: ['01', '月', '01', '日', '23', ':', '50']\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   *** Example ***\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   idx: 0\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   guid: 4456072029125500\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   tokens: [CLS] 写 在 年 末 冬 初 孩 子 流 感 的 第 五 天 ， 我 们 仍 然 没 有 忘 记 热 情 拥 抱 这 2020 年 的 第 一 天 。 带 着 一 丝 迷 信 ， 早 晨 给 孩 子 穿 上 红 色 的 羽 绒 服 羽 绒 裤 ， 祈 祷 新 的 一 年 ， 孩 子 们 身 体 康 健 。 仍 然 会 有 一 丝 焦 虑 ， 焦 虑 我 的 孩 子 为 什 么 会 过 早 的 懂 事 ， 从 两 岁 多 开 始 关 注 我 的 情 绪 ， 会 深 沉 地 说 ： 妈 妈 ， 你 终 于 笑 了 ！ 这 句 话 像 刀 子 一 样 扎 入 我 ? 展 开 全 文 c [SEP] 01 月 01 日 23 : [SEP]\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   input_ids: 101 1091 1762 2399 3314 1100 1159 2111 2094 3837 2697 4638 5018 758 1921 8024 2769 812 793 4197 3766 3300 2563 6381 4178 2658 2881 2849 6821 8439 2399 4638 5018 671 1921 511 2372 4708 671 692 6837 928 8024 3193 3247 5314 2111 2094 4959 677 5273 5682 4638 5417 5309 3302 5417 5309 6175 8024 4857 4876 3173 4638 671 2399 8024 2111 2094 812 6716 860 2434 978 511 793 4197 833 3300 671 692 4193 5991 8024 4193 5991 2769 4638 2111 2094 711 784 720 833 6814 3193 4638 2743 752 8024 794 697 2259 1914 2458 1993 1068 3800 2769 4638 2658 5328 8024 833 3918 3756 1765 6432 8038 1968 1968 8024 872 5303 754 5010 749 8013 6821 1368 6413 1008 1143 2094 671 3416 2799 1057 2769 136 2245 2458 1059 3152 145 102 8146 3299 8146 3189 8133 131 102 0 0\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 14:36:12 - INFO - __main__ -   label: 1\n",
      "04/02/2020 14:39:17 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2020 14:39:17 - INFO - __main__ -     Num examples = 79929\n",
      "04/02/2020 14:39:17 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:39:17 - INFO - __main__ -     Num steps = 2500\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]04/02/2020 14:40:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:40:04 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:40:04 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:41:06 - INFO - __main__ -     eval_F1 = 0.29127233543691555\n",
      "04/02/2020 14:41:06 - INFO - __main__ -     eval_loss = 1.0504839551715424\n",
      "04/02/2020 14:41:06 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.29127233543691555\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6483:  32%|████████▋                  | 799/2500 [04:13<05:14,  5.41it/s]04/02/2020 14:43:30 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:43:30 - INFO - __main__ -     global_step = 200\n",
      "04/02/2020 14:43:30 - INFO - __main__ -     train loss = 0.6483\n",
      "loss 0.5886:  64%|████████████████▋         | 1599/2500 [06:36<02:46,  5.41it/s]04/02/2020 14:45:53 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:45:53 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 14:45:53 - INFO - __main__ -     train loss = 0.5886\n",
      "04/02/2020 14:46:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:46:39 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:46:39 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:47:41 - INFO - __main__ -     eval_F1 = 0.7273625201675661\n",
      "04/02/2020 14:47:41 - INFO - __main__ -     eval_loss = 0.5562405790002963\n",
      "04/02/2020 14:47:41 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 14:47:41 - INFO - __main__ -     loss = 0.5886\n",
      "================================================================================\n",
      "Best F1 0.7273625201675661\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5547:  96%|████████████████████████▉ | 2399/2500 [10:49<00:18,  5.40it/s]04/02/2020 14:50:07 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:50:07 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 14:50:07 - INFO - __main__ -     train loss = 0.5547\n",
      "04/02/2020 14:50:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:50:53 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:50:53 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 14:51:55 - INFO - __main__ -     eval_F1 = 0.7246793740627414\n",
      "04/02/2020 14:51:55 - INFO - __main__ -     eval_loss = 0.5471432026678\n",
      "04/02/2020 14:51:55 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 14:51:55 - INFO - __main__ -     loss = 0.5547\n",
      "================================================================================\n",
      "loss 0.5736: 100%|██████████████████████████| 2500/2500 [12:55<00:00,  3.22it/s]\n",
      "04/02/2020 14:52:14 - INFO - pytorch_transformers.modeling_utils -   loading weights file data/model_save/roberta_wwm_large_48_gru1_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7273625201675661\n",
      "/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ./bert_last3embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_48/data_origin_2 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_48_gru1_2 \\\n",
    "--max_seq_length 155 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 64 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 2500 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 13:41:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   Model name '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   loading file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "04/02/2020 13:41:08 - INFO - pytorch_transformers.modeling_utils -   loading weights file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "04/02/2020 13:41:15 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2020 13:41:15 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   content: ['01', '月', '01', '日', '23', ':', '50']\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   *** Example ***\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   idx: 0\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   guid: 4456072029125500\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   tokens: [CLS] 写 在 年 末 冬 初 孩 子 流 感 的 第 五 天 ， 我 们 仍 然 没 有 忘 记 热 情 拥 抱 这 2020 年 的 第 一 天 。 带 着 一 丝 迷 信 ， 早 晨 给 孩 子 穿 上 红 色 的 羽 绒 服 羽 绒 裤 ， 祈 祷 新 的 一 年 ， 孩 子 们 身 体 康 健 。 仍 然 会 有 一 丝 焦 虑 ， 焦 虑 我 的 孩 子 为 什 么 会 过 早 的 懂 事 ， 从 两 岁 多 开 始 关 注 我 的 情 绪 ， 会 深 沉 地 说 ： 妈 妈 ， 你 终 于 笑 了 ！ 这 句 话 像 刀 子 一 样 扎 入 我 ? 展 开 全 文 c [SEP] 01 月 01 日 23 : [SEP]\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   input_ids: 101 1091 1762 2399 3314 1100 1159 2111 2094 3837 2697 4638 5018 758 1921 8024 2769 812 793 4197 3766 3300 2563 6381 4178 2658 2881 2849 6821 8439 2399 4638 5018 671 1921 511 2372 4708 671 692 6837 928 8024 3193 3247 5314 2111 2094 4959 677 5273 5682 4638 5417 5309 3302 5417 5309 6175 8024 4857 4876 3173 4638 671 2399 8024 2111 2094 812 6716 860 2434 978 511 793 4197 833 3300 671 692 4193 5991 8024 4193 5991 2769 4638 2111 2094 711 784 720 833 6814 3193 4638 2743 752 8024 794 697 2259 1914 2458 1993 1068 3800 2769 4638 2658 5328 8024 833 3918 3756 1765 6432 8038 1968 1968 8024 872 5303 754 5010 749 8013 6821 1368 6413 1008 1143 2094 671 3416 2799 1057 2769 136 2245 2458 1059 3152 145 102 8146 3299 8146 3189 8133 131 102 0 0\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      "04/02/2020 13:41:22 - INFO - __main__ -   label: 1\n",
      "04/02/2020 13:44:25 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2020 13:44:25 - INFO - __main__ -     Num examples = 79929\n",
      "04/02/2020 13:44:25 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:44:25 - INFO - __main__ -     Num steps = 2500\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]04/02/2020 13:45:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:45:11 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 13:45:11 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:46:14 - INFO - __main__ -     eval_F1 = 0.2883775928983065\n",
      "04/02/2020 13:46:14 - INFO - __main__ -     eval_loss = 1.0519140638863316\n",
      "04/02/2020 13:46:14 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.2883775928983065\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6381:  32%|████████▋                  | 799/2500 [04:13<05:20,  5.31it/s]04/02/2020 13:48:39 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:48:39 - INFO - __main__ -     global_step = 200\n",
      "04/02/2020 13:48:39 - INFO - __main__ -     train loss = 0.6381\n",
      "loss 0.5863:  64%|████████████████▋         | 1599/2500 [06:38<02:46,  5.40it/s]04/02/2020 13:51:03 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:51:03 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 13:51:03 - INFO - __main__ -     train loss = 0.5863\n",
      "04/02/2020 13:51:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:51:49 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 13:51:49 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 13:52:51 - INFO - __main__ -     eval_F1 = 0.7147191640307291\n",
      "04/02/2020 13:52:51 - INFO - __main__ -     eval_loss = 0.5645578938741653\n",
      "04/02/2020 13:52:51 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 13:52:51 - INFO - __main__ -     loss = 0.5863\n",
      "================================================================================\n",
      "Best F1 0.7147191640307291\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5617:  96%|████████████████████████▉ | 2399/2500 [10:52<00:18,  5.39it/s]04/02/2020 13:55:17 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 13:55:17 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 13:55:17 - INFO - __main__ -     train loss = 0.5617\n",
      "04/02/2020 13:56:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 13:56:03 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 13:56:03 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 13:57:05 - INFO - __main__ -     eval_F1 = 0.7231968400529186\n",
      "04/02/2020 13:57:05 - INFO - __main__ -     eval_loss = 0.5565289329416074\n",
      "04/02/2020 13:57:05 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 13:57:05 - INFO - __main__ -     loss = 0.5617\n",
      "================================================================================\n",
      "Best F1 0.7231968400529186\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5482: 100%|██████████████████████████| 2500/2500 [13:00<00:00,  3.20it/s]\n",
      "04/02/2020 13:57:27 - INFO - pytorch_transformers.modeling_utils -   loading weights file data/model_save/roberta_wwm_large_48_gru1_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7231968400529186\n",
      "/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ./bert_last3embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_5566/data_origin_3 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_48_gru1_3 \\\n",
    "--max_seq_length 155 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 64 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 2500 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/02/2020 14:00:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   Model name '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   loading file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "04/02/2020 14:00:23 - INFO - pytorch_transformers.modeling_utils -   loading weights file /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "04/02/2020 14:00:31 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/02/2020 14:00:31 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   content: ['01', '月', '01', '日', '23', ':', '58']\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   *** Example ***\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   idx: 0\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   guid: 4456074167480980\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   tokens: [CLS] 开 年 大 模 型 [UNK] 累 到 以 为 自 己 发 烧 了 腰 疼 膝 盖 疼 腿 疼 胳 膊 疼 脖 子 疼 # [UNK] 的 [UNK] # ? [SEP] 01 月 01 日 23 : [SEP]\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   input_ids: 101 2458 2399 1920 3563 1798 100 5168 1168 809 711 5632 2346 1355 4173 749 5587 4563 5607 4667 4563 5597 4563 5538 5600 4563 5556 2094 4563 108 100 4638 100 108 136 102 8146 3299 8146 3189 8133 131 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/02/2020 14:00:38 - INFO - __main__ -   label: 0\n",
      "04/02/2020 14:03:42 - INFO - __main__ -   ***** Running training *****\n",
      "04/02/2020 14:03:42 - INFO - __main__ -     Num examples = 79929\n",
      "04/02/2020 14:03:42 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:03:42 - INFO - __main__ -     Num steps = 5000\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]04/02/2020 14:04:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:04:28 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:04:28 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:05:31 - INFO - __main__ -     eval_F1 = 0.2903212511453361\n",
      "04/02/2020 14:05:31 - INFO - __main__ -     eval_loss = 1.0516101308524037\n",
      "04/02/2020 14:05:31 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.2903212511453361\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6379:  16%|████▎                      | 799/5000 [04:13<12:58,  5.39it/s]04/02/2020 14:07:55 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:07:55 - INFO - __main__ -     global_step = 200\n",
      "04/02/2020 14:07:55 - INFO - __main__ -     train loss = 0.6379\n",
      "loss 0.5879:  32%|████████▎                 | 1599/5000 [06:36<10:28,  5.41it/s]04/02/2020 14:10:19 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:10:19 - INFO - __main__ -     global_step = 400\n",
      "04/02/2020 14:10:19 - INFO - __main__ -     train loss = 0.5879\n",
      "loss 0.5628:  48%|████████████▍             | 2399/5000 [09:00<08:00,  5.41it/s]04/02/2020 14:12:42 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:12:42 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 14:12:42 - INFO - __main__ -     train loss = 0.5628\n",
      "04/02/2020 14:13:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:13:28 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:13:28 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:14:30 - INFO - __main__ -     eval_F1 = 0.7087937732864361\n",
      "04/02/2020 14:14:30 - INFO - __main__ -     eval_loss = 0.5800393427998874\n",
      "04/02/2020 14:14:30 - INFO - __main__ -     global_step = 600\n",
      "04/02/2020 14:14:30 - INFO - __main__ -     loss = 0.5628\n",
      "================================================================================\n",
      "Best F1 0.7087937732864361\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5539:  64%|████████████████▋         | 3199/5000 [13:13<05:33,  5.40it/s]04/02/2020 14:16:55 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:16:55 - INFO - __main__ -     global_step = 800\n",
      "04/02/2020 14:16:55 - INFO - __main__ -     train loss = 0.5539\n",
      "04/02/2020 14:17:41 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:17:41 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:17:41 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:18:43 - INFO - __main__ -     eval_F1 = 0.7193300846185703\n",
      "04/02/2020 14:18:43 - INFO - __main__ -     eval_loss = 0.5582471284241722\n",
      "04/02/2020 14:18:43 - INFO - __main__ -     global_step = 800\n",
      "04/02/2020 14:18:43 - INFO - __main__ -     loss = 0.5539\n",
      "================================================================================\n",
      "Best F1 0.7193300846185703\n",
      "Saving Model......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "loss 0.5552:  80%|████████████████████▊     | 3999/5000 [17:26<03:05,  5.40it/s]04/02/2020 14:21:08 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:21:08 - INFO - __main__ -     global_step = 1000\n",
      "04/02/2020 14:21:08 - INFO - __main__ -     train loss = 0.5552\n",
      "04/02/2020 14:21:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:21:54 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:21:54 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:22:56 - INFO - __main__ -     eval_F1 = 0.7264775756486821\n",
      "04/02/2020 14:22:56 - INFO - __main__ -     eval_loss = 0.5479369870009133\n",
      "04/02/2020 14:22:56 - INFO - __main__ -     global_step = 1000\n",
      "04/02/2020 14:22:56 - INFO - __main__ -     loss = 0.5552\n",
      "================================================================================\n",
      "Best F1 0.7264775756486821\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.5393:  96%|████████████████████████▉ | 4799/5000 [21:40<00:37,  5.41it/s]04/02/2020 14:25:22 - INFO - __main__ -   ***** Report result *****\n",
      "04/02/2020 14:25:22 - INFO - __main__ -     global_step = 1200\n",
      "04/02/2020 14:25:22 - INFO - __main__ -     train loss = 0.5393\n",
      "04/02/2020 14:26:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "04/02/2020 14:26:08 - INFO - __main__ -     Num examples = 19981\n",
      "04/02/2020 14:26:08 - INFO - __main__ -     Batch size = 64\n",
      "04/02/2020 14:27:10 - INFO - __main__ -     eval_F1 = 0.7261212106005365\n",
      "04/02/2020 14:27:10 - INFO - __main__ -     eval_loss = 0.5439555268889418\n",
      "04/02/2020 14:27:10 - INFO - __main__ -     global_step = 1200\n",
      "04/02/2020 14:27:10 - INFO - __main__ -     loss = 0.5393\n",
      "================================================================================\n",
      "loss 0.5353: 100%|██████████████████████████| 5000/5000 [24:04<00:00,  3.46it/s]\n",
      "04/02/2020 14:27:47 - INFO - pytorch_transformers.modeling_utils -   loading weights file data/model_save/roberta_wwm_large_48_gru1_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7264775756486821\n",
      "/appcom/AnacondaInstall/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ./bert_last3embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_48/data_origin_4 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_48_gru1_4 \\\n",
    "--max_seq_length 155 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 64 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 5000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train = pd.read_csv('data/nCoV_100k_train.labled.csv', header=0)\n",
    "test = pd.read_csv('data/nCov_10k_test.csv', header=0)\n",
    "train = train[(train['情感倾向'].isin(['-1', '0', '1']))]\n",
    "train = train[~train.index.isin(['35202', '41624', '33936'])].reset_index(drop=True)\n",
    "test = pd.read_csv('data/nCov_10k_test.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_48_gru1_0/'\n",
    "sub = pd.read_csv(path + 'sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.columns\n",
    "sub1_col = [ 'label_0', 'label_1', 'label_2']\n",
    "test['y'] = np.argmax(sub[sub1_col].values, axis=1) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_best = np.loadtxt('data/model_save2/test_bert_prob_0.7354800953858035.txt')\n",
    "test['y2'] = np.argmax(test_best, axis=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.5667\n",
       " 1    0.2645\n",
       "-1    0.1688\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['y'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9207"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['y'] == test['y2']].shape[0]/test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博中文内容</th>\n",
       "      <th>y</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>恋家的宝贝//@1215theCloudsilent_云默:回家就好了</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>人生的第一次期末考试前，他发烧了。?</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>事实证明人在发烧时表现出来的才是最真实的自己。?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>其實在拍照的時候我有點小發燒我這真的是燃燒??生命、用愛??發電?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9965</th>\n",
       "      <td>打造“高效、畅顺、平安、智慧、温馨春运”！广宁公安全力以赴保障春运交通安全1月2日下午，广宁...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>【#火神山医院连通远程会诊系统#】今天，解放军总医院与武汉火神山医院远程会诊系统实现连通，远...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>找点乐子还得过下去//@老李家最有钱的仔叫李大亮:！//@宋英杰:以前没有火神山和雷神山？</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>黑鹰坠落。谁在任时买的？那个党派？会不会成为选票拐点？?</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>心灵鸡汤#武汉加油#我们所有人，和我们这个国家一起，正在经历着一场这个星球上史无前例的考验...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1069 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 微博中文内容  y  y2\n",
       "1     大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情... -1   0\n",
       "13                  恋家的宝贝//@1215theCloudsilent_云默:回家就好了  0   1\n",
       "17                                   人生的第一次期末考试前，他发烧了。?  0  -1\n",
       "26                             事实证明人在发烧时表现出来的才是最真实的自己。?  1   0\n",
       "27                    其實在拍照的時候我有點小發燒我這真的是燃燒??生命、用愛??發電?  1   0\n",
       "...                                                 ... ..  ..\n",
       "9965  打造“高效、畅顺、平安、智慧、温馨春运”！广宁公安全力以赴保障春运交通安全1月2日下午，广宁...  0   1\n",
       "9982  【#火神山医院连通远程会诊系统#】今天，解放军总医院与武汉火神山医院远程会诊系统实现连通，远...  1   0\n",
       "9988      找点乐子还得过下去//@老李家最有钱的仔叫李大亮:！//@宋英杰:以前没有火神山和雷神山？  1   0\n",
       "9991                       黑鹰坠落。谁在任时买的？那个党派？会不会成为选票拐点？?  0  -1\n",
       "9996  心灵鸡汤#武汉加油#我们所有人，和我们这个国家一起，正在经历着一场这个星球上史无前例的考验...  1   0\n",
       "\n",
       "[1069 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['y'] != test['y2']][['微博中文内容','y','y2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !python ./bert_pytorch_base.py \\\n",
    "# --model_type bert \\\n",
    "# --model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "# --do_train \\\n",
    "# --do_eval \\\n",
    "# --do_test \\\n",
    "# --data_dir /data/data01/liyang099/com/multi_setiment_reg/data/data_StratifiedKFold_5566/data_origin_0 \\\n",
    "# --output_dir data/model_save/roberta_wwm_large_2562_gru1_0 \\\n",
    "# --max_seq_length 150 \\\n",
    "# --split_num 2 \\\n",
    "# --lstm_hidden_size 512 \\\n",
    "# --lstm_layers 1 \\\n",
    "# --lstm_dropout 0.1 \\\n",
    "# --eval_steps 200 \\\n",
    "# --per_gpu_train_batch_size 64 \\\n",
    "# --gradient_accumulation_steps 4 \\\n",
    "# --warmup_steps 0 \\\n",
    "# --per_gpu_eval_batch_size 64 \\\n",
    "# --learning_rate 5e-5 \\\n",
    "# --adam_epsilon 1e-6 \\\n",
    "# --weight_decay 0 \\\n",
    "# --train_steps 9000 \\\n",
    "# --freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python ./bert_pytorch_base.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path /data/data01/liyang099/com/weight/torch/chinese_roberta_wwm_ext_pytorch/  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir /data/data01/liyang099/com/setiment_regnition/data/data_StratifiedKFold_5566/data_origin_1 \\\n",
    "--output_dir data/model_save/roberta_wwm_large_2562_gru1_0 \\\n",
    "--max_seq_length 150 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 64 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 9000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_0/sub.csv')\n",
    "oof_test = test[['label_0','label_1','label_2']].values\n",
    "test['y'] = np.argmax(oof_test, axis=1) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_0/sub.csv')\n",
    "test2 = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_1/sub.csv')\n",
    "test3 = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_2/sub.csv')\n",
    "test4 = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_3/sub.csv')\n",
    "test5 = pd.read_csv('/data/data01/liyang099/com/multi_setiment_reg/data/model_save/roberta_wwm_large_2562_gru1_4/sub.csv')\n",
    "oof_test1 = test1[['label_0','label_1','label_2']].values\n",
    "oof_test2 = test2[['label_0','label_1','label_2']].values\n",
    "oof_test3 = test3[['label_0','label_1','label_2']].values\n",
    "oof_test4 = test4[['label_0','label_1','label_2']].values\n",
    "oof_test5 = test5[['label_0','label_1','label_2']].values\n",
    "oof_test = (oof_test1+ oof_test2 + oof_test3 + oof_test4 +oof_test5)/5\n",
    "test['y'] = np.argmax(oof_test, axis=1) -1\n",
    "test.rename(columns={'微博id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_test = oof_test1+ oof_test2 + oof_test3 + oof_test4 +oof_test5\n",
    "test['y'] = np.argmax(oof_test, axis=1) -1\n",
    "test.rename(columns={'微博id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>微博发布时间</th>\n",
       "      <th>发布人账号</th>\n",
       "      <th>微博中文内容</th>\n",
       "      <th>微博图片</th>\n",
       "      <th>微博视频</th>\n",
       "      <th>y</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>01月01日 23:38</td>\n",
       "      <td>-精緻的豬豬女戰士-</td>\n",
       "      <td>#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>01月02日 23:09</td>\n",
       "      <td>liujunyi88</td>\n",
       "      <td>大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id        微博发布时间       发布人账号  \\\n",
       "0  4456068992182160  01月01日 23:38  -精緻的豬豬女戰士-   \n",
       "1  4456424178427250  01月02日 23:09  liujunyi88   \n",
       "\n",
       "                                              微博中文内容  \\\n",
       "0  #你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...   \n",
       "1  大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...   \n",
       "\n",
       "                                                微博图片 微博视频  y  y2  \n",
       "0  ['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...   []  0   0  \n",
       "1                                                 []   []  0   0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[['id', 'y']].to_csv('/data/data01/liyang099/com/multi_setiment_reg/data/submit/submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test2 = pd.read_csv('/data/data01/liyang099/com/setiment_regnition/data/submit/bert_0.717071140841614.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.5924\n",
       " 1    0.2492\n",
       "-1    0.1584\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['y'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 2, 2, 0, 0, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients = [1.0,1.0,0.65]\n",
    "oof_test2= oof_test * coefficients\n",
    "test['y'] = np.argmax(oof_test2, axis=1) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.5602\n",
       " 1    0.2801\n",
       "-1    0.1597\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['y'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@Time ： 2020/3/15 22:59\n",
    "@Auth ： joleo\n",
    "@File ：optimized_rounder.py\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    # https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _macro_f1_score(y_true, y_pred, n_labels):\n",
    "        # https://www.kaggle.com/corochann/fast-macro-f1-computation\n",
    "        total_f1 = 0.\n",
    "        for i in range(n_labels):\n",
    "            yt = y_true == i\n",
    "            yp = y_pred == i\n",
    "\n",
    "            tp = np.sum(yt & yp)\n",
    "\n",
    "            tpfp = np.sum(yp)\n",
    "            tpfn = np.sum(yt)\n",
    "            if tpfp == 0:\n",
    "                print('[WARNING] F-score is ill-defined and being set to 0.0 in labels with no predicted samples.')\n",
    "                precision = 0.\n",
    "            else:\n",
    "                precision = tp / tpfp\n",
    "            if tpfn == 0:\n",
    "                print(f'[ERROR] label not found in y_true...')\n",
    "                recall = 0.\n",
    "            else:\n",
    "                recall = tp / tpfn\n",
    "            if precision == 0. or recall == 0.:\n",
    "                f1 = 0.\n",
    "            else:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "            total_f1 += f1\n",
    "        return total_f1 / n_labels\n",
    "\n",
    "    def _f1_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = coef * np.copy(X)\n",
    "\n",
    "        return -f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._f1_loss, X=X, y=y)\n",
    "        initial_coef = [1.0,1.1,1.0]#[1.0 for _ in range(len(set(y)))]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "\n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        X_p = self.coef_['x'] * np.copy(X)\n",
    "\n",
    "        return f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
